from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import precision_recall_curve
from sklearn.model_selection import RepeatedKFold
from sklearn.utils import resample

from mpl_toolkits.mplot3d import Axes3D

import pymatgen as pmg
from monty.serialization import dumpfn

import pandas as pd
import seaborn as sns
import os
import pickle

import numpy as np
import matplotlib.pyplot as plt

"""
This module provides classes for training, testing, and deploying a PU learning model for predicting material synthesizability. Utility functions for plotting aid in visualizing and analyzing results.

References:
    [1] DOI: 10.1021/acsnano.8b08014
    [2] DOI: 10.1145/1401890.1401920
    [3] DOI: 10.1016/j.patrec.2013.06.010
"""

__author__ = "Nathan C. Frey, Jin Wang"
__copyright__ = "MIT License"
__version__ = "0.1"
__maintainer__ = "Nathan C. Frey"
__email__ = "n.frey@seas.upenn.edu"
__status__ = "Development"
__date__ = "Aug 2017"

class PULearner():
    def __init__(self):
        """A machine learning model that predicts material synthesizability.

        Positive samples are experimentally synthesized materials. Unlabled samples are not-yet synthesized materials that are part of the same material family.

        Features for training data might be generated by first principles (density functional theory) calculations.

        Hyperparameters are initialized with sensible defaults, but any newly trained model should have hyperparams carefully converged.

        Args:


        Attributes:
            pu_stats (dict): Outputs of cv_baggingDT

        """

    def cv_baggingDT(self, pu_data, splits=3, repeats=100, bags=100, filename=''):
        """
        Train bagged decision tree base classifiers and do repeated 
        k-fold CV.

        Synthesizability scores (0 = not synthesizable, 1 = already
        synthesized) are generated for an unlabeled sample by averaging
        the scores from the ensemble of decision tree classifiers that
        have not been trained on that sample. 

        Args:
            pu_data (json): A file of numeric features describing materials. There MUST be a column called "PU_label" where a 1 value indicates a synthesized (positive) compound and a 0 value indicates an unlabeled compound.

            splits (int): Number of splits in k-fold CV.
            repeats (int): Number of repeated k-fold CV.
            bags (int): Number of bags in bootstrap aggregation.
            filename (string): Save model training results to file with
                filename ending in .json or .pkl.

        Returns:
            pu_stats (dict): Metrics and outputs of PU learning model
                training.

        """

        # Preprocess data
        df = pd.read_json(pu_data)
        X_P, X_U = self._process_pu_data(df)

        # Split data into training and test splits for k-fold CV
        kfold = RepeatedKFold(n_splits=splits, n_repeats=repeats, 
            random_state=42)

        # Scores for PU learning (tpr = True Positive Rate)
        scores = []
        tprs = []

        # Predicted synthesis probabilty of CVed P and U sets
        prob_P = np.ones(shape=(X_P.shape[0], splits * repeats))
        prob_U = -np.ones(shape=(X_U.shape[0], splits  *repeats))

        # Feature importance
        feat_rank = np.zeros(shape=(X_P.shape[1], splits*repeats))

        idsp = 0  # index of repeated k splits

        # Loop over P and U training/test samples
        for (ptrain, ptest), (utrain, utest) in zip(kfold.split(X_P), kfold.split(X_U)):

            # Number of P and U training samples
            N_ptrain = X_P[ptrain].shape[0]
            N_utrain = X_U[utrain].shape[0]


            d = X_P.shape[1]
            K = N_ptrain
            train_label = np.zeros(shape=(N_ptrain + K,))
            train_label[:N_ptrain] = 1.0  # Synthesized (positive)
            
            # Out of bag samples
            n_oob = np.zeros(shape=(N_utrain,))
            f_oob = np.zeros(shape=(N_utrain, 2))

            # Sums of probabilities of test sets
            f_ptest = np.zeros(shape=(X_P[ptest].shape[0], 2))
            f_utest = np.zeros(shape=(X_U[utest].shape[0], 2))

            # Bootstrap resampling for each bag
            for i in range(bags):
                bootstrap_sample = np.random.choice(np.arange(N_utrain), replace=True, size=K)

                # Positive samples and bootstrapped unlabeled samples
                data_bootstrap = np.concatenate((X_P[ptrain], X_U[bootstrap_sample, :]), axis=0)

                # Train decision tree classifier
                model = DecisionTreeClassifier(max_depth=None, max_features=None, criterion='gini', class_weight='balanced')

                model.fit(data_bootstrap, train_label)

                # Index for the oob samples
                idx_oob = sorted(set(range(N_utrain)) - set(np.unique(bootstrap_sample)))

                # Transductive learning on oob samples
                f_oob[idx_oob] += model.predict_proba(X_U[utrain][idx_oob])
                n_oob[idx_oob] += 1
                f_ptest += model.predict_proba(X_P[ptest])
                f_utest += model.predict_proba(X_U[utest])
                feat_rank[:,idsp] = model.feature_importances_

            # Predicted synthesis probabilities of unlabeled samples
            predict_utrain = f_oob[:, 1] / n_oob

            # Predicted probabilities for P and U test sets
            predict_ptest = f_ptest[:, 1] / bags
            predict_utest = f_utest[:, 1] / bags

            # Find predicted positives
            true_pos = predict_ptest[np.where(predict_ptest > 0.5)].shape[0]
            u_pos = predict_utest[np.where(predict_utest > 0.5)].shape[0]

            N_ptest = X_P[ptest].shape[0]
            N_utest = X_U[utest].shape[0]

            # Predicted positive ratio in test set
            p_pred_pos = (true_pos + u_pos)/(N_ptest + N_utest) + 0.0001

            # Compute PU recall (TPR) and score metrics
            recall = true_pos / N_ptest
            score = recall**2 / p_pred_pos
            scores.append(score)
            tprs.append(recall)

            # Predicted probabilities
            prob_P[ptest, idsp] = predict_ptest
            prob_U[utrain, idsp] = predict_utrain
            prob_U[utest, idsp] = predict_utest
            idsp += 1

            # Progress update
            if (idsp + 1) % splits == 0:
                tpr_tmp = np.asarray(tprs[-splits - 1: -1])
                print("Performed Repeated " + str(splits) + "-fold: " + str(idsp//splits + 1) + " out of " + str(repeats))
                print("True Positive Rate: %0.2f (+/- %0.2f)" % (tpr_tmp.mean(), tpr_tmp.std() * 2))

        # Predicted labels from k-fold CV
        label_U = np.zeros(shape=(X_U.shape[0], splits * repeats + 1),dtype=int)
        label_U[:, :splits * repeats][np.where(prob_U > 0.5)] = 1
        label_U[:,splits * repeats] = np.sum(label_U[:, :splits*repeats + 1],axis=1)

        tprs = np.asarray(tprs)
        scores = np.asarray(scores)

        # Metrics for each model in the k-folds
        label_U_rp = np.zeros(shape=(X_U.shape[0], repeats), dtype=int)
        prob_U_rp = np.zeros(shape=(X_U.shape[0], repeats))
        feat_rank_rp = np.zeros(shape=(X_U.shape[1], repeats))
        tpr_rp = np.zeros(shape=(repeats,))
        scores_rp = np.zeros(shape=(repeats,))
        labels = np.zeros(shape=(X_U.shape[0],))
        
        for i in range(repeats):
            prob_U_rp[:,i] = prob_U[:,i * splits:(i+1) * splits].mean(axis=1)
            feat_rank_rp[:,i] = feat_rank[:, i * splits:(i+1) * splits].mean(axis=1)
            tpr_rp[i] = tprs[i * splits:(i+1) * splits].mean()
            scores_rp[i]=scores[i * splits:(i+1) * splits].mean()

        label_U_rp[np.where(prob_U_rp > 0.5)] = 1
        prob = prob_U_rp.mean(axis=1)
        labels[np.where(prob > 0.5)] = 1

        # Get confidence interval of TPR for each kfold
        tpr_low, tpr_up = self.bootstrapCI(tpr_rp)
        scores_low, scores_up = self.bootstrapCI(scores_rp)

        # PU learning metrics
        metrics = np.asarray([tpr_rp.mean(), tpr_low, tpr_up, 
                              scores_rp.mean(), scores_low, scores_up])

        print("Accuracy: %0.2f" % (tpr_rp.mean()))
        print("95%% confidence interval: [%0.2f, %0.2f]"  % (tpr_low, tpr_up))

        # Metrics and results from training / testing
        pu_stats = {'prob': prob, 'labels': labels, 'metrics': metrics,
            'prob_rp': prob_U_rp, 'label_rp': label_U_rp, 
            'tpr_rp': tpr_rp, 'scores_rp': scores_rp, 
            'feat_rank_rp': feat_rank_rp}

        # Save results
        if filename:
            if filename.endswith(".json"):
                dumpfn(pu_stats, filename)
            if filename.endswith(".pkl"):
                with open(filename, 'wb') as file:
                    pickle.dump(pu_stats, file, protocol=pickle.HIGHEST_PROTOCOL)

        self.pu_stats = pu_stats
        return pu_stats

    def bootstrapCI(self, data, ci=95, ns=10000):
        """Compute confidence interval of the TPR.

        Args:
            data (array): Array of TPRs for each kfold.
            ci (int): Confidence interval.
            ns (int): Number of bootstrap resamplings.

        Returns:
            lower (float): Lower endpoint of CI.
            upper (float): Upper endpoint of CI.
            
        """

        bs_rsample = []
        for _ in range(ns):
            rsample = resample(data, n_samples=len(data))
            bs_rsample.append(np.mean(rsample))

        bs_rsample = np.asarray(bs_rsample)
        lower = np.percentile(bs_rsample, (100 - ci) / 2)
        upper = np.percentile(bs_rsample, ci + (100 - ci) / 2)

        return lower, upper

    def _process_pu_data(self, data):
        """Utility method for processing input data.

        Args:
            data (DataFrame): Data with positive and unlabeled samples.

        Returns:
            X_P (array): Positive sample set.
            X_U (array): Unlabeled sample set.

        """

        df_P = data.query("PU_label == 1")  # Positive value is 1
        df_U = data.query("PU_label == 0")  # Unlabeled value is 0

        # Chop off PU label
        X_P = np.asarray(df_P)[:,:-1]
        X_U = np.asarray(df_U)[:,:-1]

        return X_P, X_U


class PUInteract():
    def __init__(self):
        """Consider parent and child phase PU learning scores.

        This class looks at PU learning scores for parent bulk
        compounds (e.g. layered h-BN) and scores of the child phases
        along with descriptors like exfoliation energy and changes
        in structural/electronic properties to predict (parent, child)
        pairs that can be synthesized.

        TODO:
            * Implement

        """

        pass 


class PUOutput():
    def __init__(self, pu_stats):
        """Process output from PU learning k-fold cross validation.

        This class processes the output into human readable format
        and assists in visualization of results.

        Args:
            pu_stats (dict): Output from PULearner.cv_baggingDT

        """

        pass